---
layout: post
title:  "深度学习入门（五）多层感知机与bp算法"
date:   2018-04-04
categories: 深度学习入门
excerpt: 嗯
---
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=default" id=""></script>

环境：macOS-10.13.4, Python-3.6, Tensorflow-1.4.1

# 很厉害的Optimizer图

来源于网络.....侵删

![image](/img/optimizer.gif)

[很厉害的optimizer图下载](/img/optimizer.gif)

# 多层感知机

什么是多层感知机呢？在你懵比之前，先看一下下面这个图。

![image](/img/dl3.png)

（别问我图是用什么软件画的，我是用ppt画的，没错，就是powerpoint）

这个就是一个多层感知机的结构，看不懂没有关系，我告诉你这个是做什么用的。

首先它有三层，输入层Input Layer，隐含层Hidden Layer，输出层Output Layer。

我们现在呢，假设你有一亿个南半球人和北半球人的身高、体重、肤色。也就是说你有两亿个三维度数据，和两亿个标签，一一对应这些数据，标记他们是哪里人。

你想解决一个什么问题呢？下一次再来一个未知地区的人的身高、体重、肤色，你能较为准确的知道这个人是南半球的还是北半球的。

我们如下解决这个问题：

Step1: 构建一个两亿行，三列的矩阵，我们叫它X

Step2: 我们定义一个\\(y_1\\)表示南半球的得分，\\(y_2\\)表示北半球的得分

Step3: 我们想要充分利用X的数据，于是先假设得到一个\\(y_1\\)和\\(y_2\\)的表达式，例如\\([y_1, y_2]^{T} = f(kX + b)\\)

Step4: 我们把X输入到上述的表达式里，一行一行的来，每一行算一个\\(y_1,y_2\\)，然后和真实的标签值\\([0,1],or,[1,0]\\)进行对比，对比完之后调整k和b的值，往他们的误差更小的方向上调。一直调到X的最后一行

Step4: 保存Step4训练得到的k和b，然后输入一个新的三元组，例如[100, 50, 30]，然后就会返回一个二元组\\([y_1,y_2]\\)，之后我们只需要比较一下\\(y_1,y_2\\)谁大就行了，前者大就是南半球人，后者大就是北半球人

那么我们这个思路的核心在哪呢？就在Step3和Step4这里，如何确定一个f，如何确定一个误差，以及如何减少一个误差。

如何确定一个f呢？这个就是神经网络的激励函数。

如何确定一个误差和减少误差呢？这个就是上一节说的损失函数和优化器。

那上面那个图的结构到底是什么？

那个结构表示数据如何在神经网络中进行计算。箭头表示的就是\\(f(w*x+b)\\)的过程，里面的每一个小圆圈是一个神经元，表示是存储+计算+处理单独的一个数字！

输入节点就是每一行输入的数据，比如上面的例子每一行输入三列数，输入节点就是三个，去接收数据，准备被箭头送到下一层的神经元。

输出节点就是最终要得到的数据，比如上面的例子是一个二分类，所以输出节点就是两个，去输出数据。

对每一行的训练来说，每一次输出的数据就是两个，一个两亿行数据训练两亿次，也就是这个网络输入输出要进行两亿次，中间不停在学习的究竟是什么呢，其实就是每一步神经元上进行的计算的参数，可以是w也可以是b，它们往往也是一个矩阵，由很多的元素组成。

由于我们想更多的拟合，所以往往加入很多的隐含层，包括一个以上隐藏层的神经网络，就叫做多层感知机。

# bp算法

上面我们只搞懂了什么是神经网络，什么是多层感知机，有一个遗留问题没有解决。

那就是究竟如何确定一个误差和减少误差呢？如何使用损失函数和优化器进行优化？

比如上述的网络，在某次训练中输入了三个数，最终输出了两个数，和标签的两个数对比，然后不就结束了么，少了一个调参的过程....所以自然而然，就让输出层，再把结果反馈回去，作用在参数上，就好了呀。

这也就是bp算法，全称back propagation，也就是反向传播。

反向传播+训练参数的过程可以直接参考这篇文章：[https://zhuanlan.zhihu.com/p/24801814](https://zhuanlan.zhihu.com/p/24801814)

这里也就描述一下大体步骤：

Step1: 走一波正常的网络，达到输出层

Step2: 取得真实的数据标签，计算loss

Step3: 将loss沿着神经网络反向传回去，作用回到原来带参数的神经元，计算各种对参数的偏导，计算的过程和定义的optimizer一致，修改参数。

Step4: Step3看不懂的话先看一下上一节的loss function 与 optimizer

当然，5月3号我已经在第十篇里将bp的核心公式都推导了一遍，传送门点开即可食用：

[http://www.ecohnoch.cn/2018/05/03/shuxue69/](http://www.ecohnoch.cn/2018/05/03/shuxue69/)


# 其他

多层感知机是用的最广泛的神经网络结构，bp算法是用的最多的神经网络算法，尤其是他们结合起来的东西也是用的最多的。

主要都用在分类和预测上，继而就能用在自动控制，xx识别，xx拟合等。

本博客中讲的深度学习的第二篇的实例说到底就是多层感知机+bp的结合。






