---
layout: post
title:  "深度度量学习（二）协方差、Mahalanobis距离与Euclidean距离"
date:   2019-06-24
categories: 算法与数学
excerpt: 嗯
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]},
        messageStyle: "none"
    });
</script>

原创：岐山凤鸣，转载请注明本站域名

觉得不错不妨star/follow一下我的Github

参考：

```
Kulis, Brian. (2012). Metric Learning: A Survey. Foundations and Trends in Machine Learning. 5. 10.1561/2200000019. 
```

这一篇主要说明两种距离，马氏距离(Mahalanobis Distance)和欧几里得距离(Euclidean Distance)。

以及度量学习相关的符号表示。

其中马氏距离基于协方差矩阵，所以对协方差和协方差矩阵的理解和相关计算的掌握是非常必要的。

并且本博文增加相关参考和原创的代码来进一步描述。


# 协方差 和 协方差矩阵

回顾基础的数学课是因为除了运算和会去分析，还需要理解其几何含义和代数含义，这样才能更方便在深层特征空间中去理解特征代数。

先从大家都耳熟能详会算能算的公式出发，设X和Y为两个随机变量，则其的协方差为：

$ Cov(X, Y) = E[(X-{\mu}_x) (Y-{\mu}_y)] = \frac{1}{n-1} \sum_{i=1}^{n}{(x_i - \bar{x})(y_i - \bar{y})} $

直接理解来看，是两个随机变量与其均值差的积的均值。

当X与Y相同的时候，那么就会变成方差$$ Cov(X, X) = E[(X - {\mu}_x)^2] = \frac{1}{n-1} \sum_{i=1}^{n}{(x_i - \bar{x})^2} $$，方差的小学也学过，其含义就是和样本均值的一个波动性，也就是样本中偏离均值的一个度量程度。

对一系列n个随机变量$$X_1, ..., X_n$$来说，其中任意两个随机变量$$X_i, X_j$$的协方差，形成的矩阵即为协方差矩阵。

很简单的定义，计算也很简单，大二的时候便会学，那么接下来要理解的是其在度量学习中的意义。

我们从概率统计回到线性空间，在之前定义过内积的线性空间中，我们考虑过引入内积后可以定义正交，即向量垂直的概念，同时，定义内积计算也可以定义出角度。考虑在线性空间中对内积进行角度的计算：

$ \vec a \cdot \vec b = \|\vec a\| \|\vec b\| \cos{\theta} $

推算出两个向量余弦角度为：

$  \cos{\theta} = \frac{A \cdot B}{\|A\|\|B\|} =  \frac{\sum_{i=1}^{n}{A_i*B_i}}{ \sqrt{ \sum_{i=1}^{n}{(A_i)^2} } * \sqrt{ \sum_{i=1}^{n}{(B_i)^2} }}$

可以看到分子上是向量内积，再结合协方差的公式，将随机变量X和Y表示为向量，然后【进行零均值化后】，则可以发现：

$ Cov(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n}{(x_i - \bar{x})(y_i - \bar{y})} = \frac{\vec x \cdot \vec y}{n-1}$

其中对$$ \vec x, \vec y$$均是进行0均值化的向量，对原始的随机变量，哪怕它们正交，但并不意味着协方差为0，所以对向量要根据自己的均值需要一个相对的变换，得到的向量均值要为0，这种情况下协方差才可以作为一种内积的表现。为了消去向量数值的影响，所以一般从内积除去向量长度即为余弦距离，而对于随机变量得到的向量，其角度便是耳熟能详的相关系数。

$ r = \frac{Cov(X, Y)}{\sqrt{Var(X)} * \sqrt{Var(Y)}} = \frac{\sum_{i=1}^{n}{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum_{i=1}^{n}{(X_i - \bar{X})^2}} * \sqrt{\sum_{i=1}^{n}{(Y_i - \bar{Y})^2}}} $

有一些很有用的性质，很多人拿去水数模比赛。

* $$0<r<=1, or Cov(X,Y) > 0$$，正相关
* $$-1<=r<0, or Cov(X,Y) < 0$$，负相关
* $$r=0, or Cov(X,Y)=0$$，不相关，但不代表独立

说这么多其实就是把协方差矩阵和度量空间联系一下，对各种随机独立变量来说，一组和一组之间如何消除量纲去比较其中的相关性，就是余弦距离和相关系数的作用，而协方差矩阵则更具体的表明了向量两两之间的相关性。

这个并不是说要会用这些工具去水比赛或者水一些东西，这个在之后距离运算以及和概率的理解中是会遇到很多的，很有用，理解其几何意义和代数意义是必要的。

# Mahalanobis距离

直接上公式，对向量$$x_i, x_j$$，其Mahalanobis距离为：

$ d_{mahalanobis}(x_i, x_j) = \sqrt{(x_i - x_j)^T {\sum}^{-1} (x_i - x_j)} $

