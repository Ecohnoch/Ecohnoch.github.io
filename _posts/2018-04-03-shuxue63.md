---
layout: post
title:  "深度学习入门（四）loss function与optimizer"
date:   2018-04-03
categories: 算法与数学
excerpt: 嗯
---
原创：岐山凤鸣

在正式讲一些网络之前，先讲这两个重要的概念，是可以直接影响到你的训练结果的。

在看这里的同学，可以先学一下简单机器学习里的线性回归、决策树、逻辑回归、随机森林和SVM。

# Loss Function (损失函数)

一个简单的神经网络得到的结果，要么就是分类，要么就是预测，要么就是什么什么。总之，既然叫深度学习，就自然有学习的过程，学习的过程说到底，就是你把模型建好之后，输入学习的数据，训练参数。loss function就是根据真实的数据标签或者什么其他的东西，来评价你每一次训练得到的参数，在一次次的训练中，降低loss function的值，你的学习就越来越好啦。

所以某次训练\-\>得到了一组训练参数\-\>loss function评价这组参数\-\>调整参数降低loss function\-\>下一次训练

评价参数是loss function，降低loss function的东西叫做optimizer优化器。

关于损失函数一般形式可以参考这一篇，讲的很详细：[https://www.zhihu.com/question/52398145](https://www.zhihu.com/question/52398145)

损失韩式一般表达式是:

![image](/img/dl1.png)

这个严格来说是目标函数，不过我们的目的都是用optimizer去优化它，就能达到我们的目的，看不懂没关系，它由两部分组成，误差值和正则化。

* 误差值

这个应该才是真正的损失函数。

先说我们有什么，我们有N个已知的标签yi，N个神经网络输出的f(xi)，还有他们的误差函数L。 （如果不知道标签是什么意思的话先看一下机器学习的无监督学习和有监督学习）

这个L最简单的就是两者相减，就能得到它们相差多少，其他的比如做个对数啊，平方差啊，一会再说。

所以前面的误差值就相当于真实值和训练值之间产生的一个平均误差，比如两者差的均值，差的平方的均值，差的对数的均值等等。

这个L有哪些种类呢？

（对分类问题）

1. 0-1损失
2. Log损失
3. Hinge损失(用于SVM)
4. 指数损失
5. 感知损失

具体的表达式过程看完这篇就知道了，这里不重复前人的工作：[https://blog.csdn.net/google19890102/article/details/50522945](https://blog.csdn.net/google19890102/article/details/50522945)

* 正则化

你设想一下，如果一昧的降低上面的误差值，最终可能过拟合，也就是为了迎合给出的训练集特意变成的那个样子。我们不希望如此。所以增加了一个正则化损失，用来强迫模型变得简单一些，不能像具体的情况那么复杂。

关于正则化也是看完这篇就知道了，这里不重复前人的工作：[https://zhuanlan.zhihu.com/p/20620638?columnSlug=mubing](https://zhuanlan.zhihu.com/p/20620638?columnSlug=mubing)

# optimizer (优化器)

上面是定义了损失函数，我们还要调整参数呢，调整参数的东西就叫做optimizer，先说一下最简单的梯度下降Gradient Descent

直接先看一下图：

![image](/img/dl2.png)

上面的是公式，就是一个简单的求导公式，下面的就是Stanford CS231n的课程中的一个演示，最左边一栏就是现在训练的一堆参数，中间就是对第一个参数加了一丢丢，前后会产生两个loss，这两个loss作差后除以加的那一丢丢值，不就是和微积分的求导的过程了一样了，能够得到一个对loss的影响趋势，所以沿梯度反方向更新参数w就能降低了。

就好比你现在本人训练了一段时间，去参加篮球比赛，得到比分s1，然后你吃了增高药，长高了一点点，假设是x，再去参加一个同样的比赛，得到了比分s2，通过(s2 - s1) / x，大概就能得到一个长高多少，就会影响分数多少的趋势。为了让自己得到0分，所以你要逆向生长，多吃垃圾食品降低身高...神经网络也是如此。

那么optimizer有多少种呢？

Tensorflow里给出了如下的一些：

* tf.train.Optimizer
* tf.train.GradientDescentOptimizer
* tf.train.AdadeltaOptimizer
* tf.train.AdagradDAOptimizer
* tf.train.MomentumOptimizer
* tf.train.AdamOptimizer
* tf.train.FtrlOptimizer
* tf.train.ProximalGradientDescentOptimizer
* tf.train.ProximalAdagradOptimizer
* tf.train.RMSPropOptimizer

具体的表达式和优缺点直接参考这一篇，这里不再赘述：[https://www.leiphone.com/news/201706/e0PuNeEzaXWsMPZX.html](https://www.leiphone.com/news/201706/e0PuNeEzaXWsMPZX.html)

