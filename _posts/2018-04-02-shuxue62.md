---
layout: post
title:  "深度学习入门（三）深度学习总览"
date:   2018-04-02
categories: 算法与数学
excerpt: 嗯
---
原创：岐山凤鸣

这几个月做深度学习相关的东西，不知不觉学会了很多的神经网络，苦于一直没有时间系统化的做一个笔记或者教程，就从这里开始吧。

这一篇就是一个总览，提供一些名词与链接。

环境：macOS-10.13.4, Python-3.6, Tensorflow-1.4.1


# 神经网络

什么是神经网络？是描述深度学习的一种模型。

神经网络的基石？大量的数据，根据大量的数据，通过神经网络找到数据的规律。

神经网络有哪些层？输入层、隐藏层、输出层。

神经网络怎么分类？最简单的就是按层数，或者按有没有监督。

神经网络用来干嘛？(图像、文本、语音)分类、识别、预测。更高级的就是人工智能。

神经网络的进阶？迁移学习（之前的工作复用），强化学习（通过反馈进行修正, Alpha Go的核心）

神经网络的基本流程？ 下面要说的就是。

# 神经网络基本流程

搭一个神经网络从头到尾怎么搞？其实很简单.....

Step1: [收集大量的数据，可以带标签的，也可以不带标签的](https://www.zhihu.com/question/53655758/answer/289133283)，得到训练集，测试集，验证集。如果听不懂这一句话的吧，可以先学习一下[简单的机器学习](https://search.bilibili.com/all?keyword=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&from_source=banner_search)

Step2: [人工构建神经网络的结构](https://blog.csdn.net/scutjy2015/article/details/74170794)，有多少个输入层结点，有多少个隐藏层，隐藏层里有多少个运算的结点。所以这个简单的结构说到底就是描述有多少层，每个层都有多少个运算结点。

Step3: [定义层与层之间的运算，也就是定义激励函数](https://www.zhihu.com/question/22334626)，是线性的还是非线性的，非线性的选哪种。

Step4: [定义loss和optimizer](https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&utm_medium=social)，如何定义损失，是做差，还是平方差，还是log差。如何优化，是梯度下降，还是随机下降。

Step5: 开始用训练集来训练，训练多少次，需不需要分步训练，需不需要训练时保存参数，需不需要训练时调用参数。

Step6: 开始用测试集来测试，测试多少次，每次测试怎么调参，需不需要考虑别的东西。

Step7: 开始用验证集来验证，仅仅验证一次，验证准确率呀，看看效果如何。

Step8: 把结构和参数保存，这样就是一个已经OK了的网络，以后可以直接调用。

# 有哪些比较经典的神经网络呢

不全，没有分类，具体遇到什么问题的时候，比如识别？分类？预测？然后看看有没有人用过下述网络做过类似的问题，有的话就认真看一下。

* [BP(反向传播网络)](https://www.cnblogs.com/charlotte77/p/5629865.html)
* [RBM(限制玻尔兹曼机)](https://blog.csdn.net/u013631121/article/details/76652647)
* [DBN(深度信念网络)](https://blog.csdn.net/u013631121/article/details/76794829)
* [DAE(降噪自动编码器)](https://blog.csdn.net/a819825294/article/details/53516980)
* [SAE(稀疏自编码器)](http://ufldl.stanford.edu/wiki/index.php/%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81%E8%87%AA%E7%BC%96%E7%A0%81%E8%A1%A8%E8%BE%BE)
* 重点！[CNN(卷积神经网络)](http://ai.51cto.com/art/201711/558921.htm)
* [FCN(全卷积网络)](https://blog.csdn.net/xiaojiajia007/article/details/54944023)
* [GAN(生成对抗网络)](https://zhuanlan.zhihu.com/p/26994666)
* [NIN(网络中网路)](https://blog.csdn.net/app_12062011/article/details/62041254)
* [RNN(递归神经网络)](https://blog.csdn.net/app_12062011/article/details/54406225)
* [LSTM(长短期记忆网络)](https://blog.csdn.net/zdy0_2004/article/details/50044879)
* [Attention(注意力机制)](https://blog.csdn.net/joshuaxx316/article/details/70665388)
* [RNTN(递归张量神经网络)](http://www.docin.com/p-1333116488.html)，自然语言处理
* [MC(马尔可夫链)](http://xueshu.baidu.com/s?wd=paperuri:(ad7dd03041519a80c26bfd07d23347fd)&filter=sc_long_sign&sc_ks_para=q%3D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8A%A0%E6%9D%83%E6%A8%A1%E7%B3%8A%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%E7%9A%84%E7%BB%84%E5%90%88%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8&tn=SE_baiduxueshu_c1gjeupa&ie=utf-8&sc_us=4330322919583965684)，自然语言处理
* [FF(前馈神经网络)](https://blog.csdn.net/qsczse943062710/article/details/61912464?locationNum=9&fps=1)
* [RBF(Radial Basis Network)](https://blog.csdn.net/huang1024rui/article/details/51510611)
* [DFF(深度前馈神经网络)](https://blog.csdn.net/u012554092/article/details/77878532)
* [HN(霍普菲尔网络)](https://wenku.baidu.com/view/98e96190bb4cf7ec4bfed02a.html)
* [LSM(液体状态机)](http://www.doc88.com/p-1354935452730.html)
* [ELM(极端学习机)](https://wenku.baidu.com/view/aea2f74303d8ce2f00662369.html)
* [ESN(回声状态网络)](https://blog.csdn.net/zwqhehe/article/details/77025035?ABstrategy=codes_snippets_optimize_v4)
* [DRN(深度残差网络)](https://blog.csdn.net/diamonjoy_zone/article/details/70904212)
* [KN(Kohonen神经网络)](http://blog.sina.com.cn/s/blog_92d2c5e10102vava.html)
* [NTM(神经图灵机)](http://www.dengfanxin.cn/?p=60)

强化学习：

* [Q-learning](https://www.zhihu.com/question/26408259)
* [Sarsa](https://blog.csdn.net/panglinzhuo/article/details/72518045)
* [DQN(Deep Q Network)](https://blog.csdn.net/gongxiaojiu/article/details/73345808)
