---
layout: post
title:  "TensorFlow 学习记录二（第一段代码例子）"
date:   2017-01-21
categories: 算法与数学
excerpt: 嗯
---

这些天忙着...哈哈，一些很幸福的事情，就没有顾着博客了。

寒假第一周过去了，也是时候开始新的学习路程。

老的那些坑，也就开始慢慢填吧。

这个新坑，就说一说TensorFlow的那些事儿～

今天就说一说上一节留下的一段代码。

```
import tensorflow as tf 
import numpy as np 

# creat data
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3

### creat tensorflow struture start
Weight = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
biases = tf.Variable(tf.zeros([1]))

y = Weight * x_data + biases
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

init = tf.initialize_all_variables()

### creat tensorflow structure end

sess = tf.Session()
sess.run(init)  # Very important

for step in range(201):
    sess.run(train)
print(step, sess.run(Weight), sess.run(biases))
```


# 首先运行看一下效果咯

在py3环境下运行，运行结果如下图。（未显示完全）

![image](http://p1.bpimg.com/1949/1d048465033311a5.png) 

第一个参数会从0，一直累加到200， 第二个参数里的值，你会发现很明显的在向0.100000**靠拢**，同样第三个参数也在向0.300000**靠拢**

于是输出结果的含义就是，我们产生了200对数字，这些数字对在这两百次里一直向着<0.10, 0.30>的方向在努力。这就是一个非常简单的训练的过程了。

试想我们现在遇到了一个需求，需要200对数据，数据的趋势如上述所说，该怎么实现呢？ 这就是这一段代码的用途了。

# 逐行解释一遍代码

这里我们来一行行解释一下代码吧。

```
import tensorflow as tf
import numpy as np
```

上面两行的意思就是导入tensorflow 和 numpy两个库，并且用tf和np来简写调用。

numpy是Python里的一个数据计算扩展，可以存储和处理大型矩阵，在下面我们就会用到它。

```
# creat data
x_data = np.random(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3
```

这两行如注释所说，是产生数据的。

我们产生了两个数据，自己编的数据，一个是x_data, 一个是y_data。

第一行np.random(100).astype(np.float32), 是随机产生了一个有100个元素，元素类型全为float32类型的数组。

第二行给定y_data 创造了一个y_data, 并且表明了y_data和x_data的关系，是一个一次函数关系。

我们希望我们产生的数据，能够经过训练，学习到这样的一个一次函数的关系。

```
### creat tensorflow structrue start
Weight = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
biases = tf.Variable(tf.zeros([1]))
```

从注释里的creat tensorflow structure start可以看出来，我们要创造一个tensorflow的结构了。两个注释中间的语句，就是整个结构了。

下面两行我们创造了两个tf变量。

Weight， 大写是因为它可能是矩阵的形式，后面的tf.Variable指的是定义的tf的变量，生成了一堆从-1到1的一维随机数列。

biases的字面意思是偏差，这里也是，其实就是上面一次函数y = x * 0.1 + 0.3中的0.3， 是作为一个偏差量，去让数据在尽量计算了前面的0.1*x 后，在加上biases，生成结果。

所以说白了，我们训练的目的，就是让Weight每次更接近0.1, biases每次更接近0.3， 这样一来，我们模拟出来的曲线，就能和y = x * 0.1 + 0.3 尽量拟合。

```
y = Weight * x_data + biases
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
```

于是这四行，就是我们训练的过程。

第一行我们再弄一个变量 y， 让它等于上面说的Weight * x_data + biases，将0.1和0.3替换成Weight和biases。

并且在第二行计算出我们新的y和y_data的一个误差，最初这个误差肯定是很大的。

所以我们在第三行定义了一个优化器optimizer，第四行用train去减少误差。

第三行中tf.train.GradientDescentOptimizer是一个最基础的优化器，传入的0.5则是我们希望优化的学习效率。

```
init = tf.initialize_all_variables()

### creat tensorflow structure end
```

这里的一行就是初始化所有的变量。

也就是我们虽然在上面建立了几个tf变量，但是我们没有初始化，所以建立的这个结构就是个死的，我们需要激活里面的变量。

```
sess = tf.Session()
sess.run(init) #Very important
```
从这里开始，就是激活我们整个结构。

session的意思就是会话控制，正如上一节点进去的官网的文档中解释的那般，在一个会话中，启动我们建好的结构。

比如这里的两行就是调用会话API来启动我们建好的结构。

很多时候我们容易忘记启动激活这个结构，也就是漏掉这个，但这个是非常非常重要的，几乎每一个代码，都会需要Session来控制会话。

```
for step in range(201):
	sess.run(train)
print(step, sess.run(Weight), sess.run(biases))
```
最后我们开始训练，进行200次训练，每次都打印出来训练的结果，每一次训练，每一次打印输出，我们都需要调用sess.run()来调用，可见其是非常重要的！

最后run这个py文档，就可以看到训练的过程了～

下一节主要就讲讲Session是个什么东西，为什么这么重要。

还是放一段代码来卖个关子。

```
import tensorflow as tf 

matrix1 = tf.constant([[3, 3]])
matrix2 = tf.constant([[2], 
	                   [2]])

product = tf.matmul(matrix1, matrix2)      # matrix multiply

# method 1
# sess = tf.Session()
# result = sess.run(product)

# print(result)
# sess.close()

# method 2
with tf.Session() as sess:
	result2 = sess.run(product)
	print(result2)
# auto close()
```


