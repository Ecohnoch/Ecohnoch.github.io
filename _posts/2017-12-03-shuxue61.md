---
layout: post
title:  "深度学习入门（三）TensorFlow系统学习第三篇，索尼离职率简单测定"
date:   2017-12-03
categories: 算法与数学
excerpt: 嗯
---
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=default" id=""></script>

最近做一个索尼公司的商业项目，要用到深度学习的内容，于是这里记录一些东西，方便查阅..

原创：岐山凤鸣

# 首先准备数据

这里我预想的是，我们先准备三列数据特征，比如职位高低的一列，年龄的一列，家庭远近的一列。三列数据均用浮点数去表示。

所以输入的x数据就是一个x行3列的矩阵，x表示输入了多少个人的数据，3列分别就是上面的三列数据。

然后标签数据就是是否离职，用0和1来表示。所以输入的y数据就是一个x行的向量，全是0或者1。

准备好数据后，做成Python的列表或者字典，等待输入。

将数据80%分为训练集，20%作为数据集，分别命名为X_train, Y_train, X_test, Y_test。

# 定义TF变量

对上述的输入数据和我们的训练对象w和b，分别定义下面的TF对象变量：

```
X = tf.placeholder(tf.float32, [None, 3])
w = tf.Variable(tf.zeros([3, 1]))
b = tf.Variable(tf.zeros([1]))
y_ = tf.placeholder(tf.float32, [None, 1])
```

X表示输入的x行3列的x矩阵数据，第一个参数表示数据类型都是float32, 第二个的None表示可以输入无限组数据，3表示每组数据是一个三元组。

w表示要训练的权重，也就是要训练的第一个参数值。

b表示要训练的偏差，也就是要训练的第二个参数值。

y_表示输入的标签数据，也就是0或1组成的向量。

# 定义激励函数和cost

这里直接线性回归有些不好，逻辑回归还不会，所以没有找到什么合适的曲线，直接用一个Relu激励函数来试试准确度，这里有待优化。

```
y = tf.nn.relu(tf.matmul(X, w) + b)
```

然后计算y和y_之间的cost，这里直接相减作平方差吧，这里也有待优化。

```
cost = tf.reduce_mean(tf.square(y_ - y))
```

# 训练

定义训练的优化器，这里也有待优化，直接用的最原始的优化器。

```
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
```

初始化全局变量和会话：

```
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run()
```

开始训练：(数据不多，有多少数据就训练多少个吧)

```
for i in range(len(X_train)):
	sess.run(train_step, feed_dict={X: X_train, Y: Y_train})
```

然后训练就结束了。

# 计算准确率

```
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(accuracy.eval({x: X_test, y_: Y_test}))
```

这里就是将测试集里的x都输入进去计算得到一堆y和已知的y对比，算一个满足了多少个的准确率。用到的都是测试集的内容。

# 所有代码

```
'''
	@author: Xiong chuyuan
	@date: 2017/12/03
'''

import tensorflow as tf 

# Data Difinition
# y = (w1x1 + w2x2 + w3x3) + b
# y = wx + b
# y: m rows matrix
# x: m * 3 matrix
# w: 3 rows matrix
X = tf.placeholder(tf.float32, [None, 3])
w = tf.Variable(tf.zeros([3, 1]))
b = tf.Variable(tf.zeros([1]))

# Relu function
y = tf.nn.relu(tf.matmul(X, w) + b)
y_ = tf.placeholder(tf.float32, [None, 1])


# Cost
cost = tf.reduce_mean(tf.square(y_ - y))

# Train, step reduce
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

# Train datas
X_train = [[1, 2, 3], [1, 2, 3], [1, 2, 3]]
y_train = [0, 0, 1]

# Train step
for i in range(len(X_train)):
	sess.run(train_step, feed_dict={X: x_train, Y: y_train})

# Test datas
X_test = [[1, 2, 3]]
Y_test = [0]

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(accuracy.eval({x: X_test, y_: Y_test}))
```